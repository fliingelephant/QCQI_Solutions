\documentclass{homeworg}

\title{QCQI Chapter 2}
\author{Zhou Huanhai}

\usepackage{braket}

\begin{document}

\maketitle

\exercise*
\[(1,-1)+(1,2)-(2,1)=(0,0)\]
Hence the three vectors are linearly dependent.

\exercise*
A matrix representation of $A$ is:

\[\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}\]

where we take $\{\ket{0},\ket{1}\}$ as the input and output bases.

If we change the output bases to $\{\ket{+},\ket{-}\}$ (input bases remains), note that $A\ket{0}=\ket{1}=\frac{\ket{+}-\ket{-}}{\sqrt{2}}$ and $A\ket{1}=\ket{0}=\frac{\ket{+}+\ket{-}}{\sqrt{2}}$, there is a different matrix representation:

\[\begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix}\]

\exercise*
For simplicity, take $\{\ket{v_i}\}$, $\{\ket{w_j}\}$ and $\{\ket{x_k}\}$ as the corresponding input and output bases.

Given that 

\[\forall i.\; A\ket{v_i}=\sum_{j}A_{ji}\ket{w_j}\]

and

\[\forall j.\; B\ket{w_j}=\sum_{k}B_{kj}\ket{x_k},\]

we have

\begin{align}
    \forall i.\; BA\ket{v_i}&=B\sum_{j}A_{ji}\ket{w_j}\nonumber\\
    &=\sum_{j}A_{ji}B\ket{w_j}\nonumber\\
    &=\sum_{j}\sum_{k}A_{ji}B_{kj}\ket{x_k}\nonumber\\
    &=\sum_{k}\sum_{j}B_{kj}A_{ji}\ket{x_k}\nonumber
\end{align}

Hence for the new operator $BA$, there is a matrix representation denoted as $(BA)_{\mathrm{rank}(X)\times \mathrm{rank}(V)}$, which satisfies:

\[\forall{k,i}.\; (BA)_{k,i}=\sum_{k}\sum_{j}B_{kj}A_{ji}\]

i.e., $(BA)_{\mathrm{rank}(X)\times \mathrm{rank}(V)}$ is the matrix product of $B_{\mathrm{rank}(X)\times \mathrm{rank}(W)}$ and $A_{\mathrm{rank}(W)\times \mathrm{rank}(V)}$.

\exercise*
Let $n$ denotes the rank of $V$ and $\{\ket{v}_i\}$ as a set of bases of $V$.

Have the identity operator $I$ written in a matrix form as follows:

\[\forall{j=0,1,2,\cdots, n}.\;I\ket{v_j}=\ket{v_j}=\sum_{i=0,1,\cdots,n}k_{ij}\ket{v_i}\]

Assume that 

\[\exists{m\neq n}.\;k_{mn}\neq0,\]

There are only two possible cases: Either $\{\ket{v_i}\}$ are linearly dependent, or $\ket{v_m}=\vec 0$,

which leads to contradiction with the fact that $\{\ket{v_i}\}$ forms a set of bases of $V$.

Therefore we have

\[\forall{i\neq j}.\;k_{ij}=0\]

and hence 

\[\forall i. k_{ii} = 1\]

So the one and only matrix representation of $I$ is $\mathrm{diag(\underbrace{1,1,1,\cdots,1}_{n})}$.

\exercise*
Let $\vec{x}=(x_1,\cdots,x_n)\in \mathbb{C}^n$, $\vec{y}=(y_1,\cdots,y_n)\in \mathbb{C}^n$ and  $\vec{z} =(z_1,\cdots,z_n)\in \mathbb{C}^n$.

Check the requirements of a inner product:
\begin{itemize}
    \item linear in the second argument:
    \begin{align}
    (\vec{x},\lambda_y \vec{y} +\lambda_z \vec{z})&=(\vec{x},(\lambda_y y_1 +\lambda_z z_1,\cdots,\lambda_y y_n +\lambda_z z_n))\nonumber\\
    &=\sum_i x_i^\star (\lambda_y y_i +\lambda_z z_i)\nonumber\\
    &=\lambda_y\sum_i x_i^\star y_i + \lambda_z\sum_i x_i^\star z_i\nonumber\\
    &=\lambda_y(\vec{x},\vec{y})+\lambda_z(\vec{x},\vec{z})\nonumber
    \end{align}
    \item Note that $\forall{a,b}\in \mathbb{C}.\;(ab)^\star=a^\star b^\star = b^\star a^\star$ and $a^\star + b^\star = (a+b)^\star$,
    
    then we have:
    \[(\vec{x},\vec{y})=\sum_i x_i^\star y_i=\sum_i (y_i^\star x_i)^\star=(\sum_i y_i^\star x_i)^\star=(\vec{y},\vec{x})^\star\]
    \item  Note that $\forall{a}\in \mathbb{C}.\;a^\star a\geq 0$ with equality if and only if $a=0$,
    
    then we have
    
    \[(\vec{x},\vec{x})=\sum_i x_i^\star x_i\geq 0,\]
    
    with equality if and only if all $x_i=0$, i.e., $\vec{x}=\vec{0}$.
    
\end{itemize}
So $(\cdot,\cdot)$ is an inner product on $\mathbb{C}^n$.

\exercise*

Let $(\cdot,\cdot)$ be inner product from $V\times V$ to $\mathbb{C}$, and $\ket{x},\ket{y},\ket{z}\in V$.

Based on the second property (conjugate-symmetry) and apply linearity in the second argument, we have
\begin{align}
    (\lambda_x\ket{x}+\lambda_y \ket{y},\ket{z})&=(\ket{z},\lambda_x\ket{x}+\lambda_y \ket{y})^\star\nonumber\\
    &=(\lambda_x(\ket{z},\ket{x})+\lambda_y(\ket{z},\ket{y}))^\star\nonumber\\
    &=\lambda_x^\star(\ket{z},\ket{x})^\star + \lambda_y^\star(\ket{z},\ket{y})^\star\nonumber\\
    &=\lambda_x^\star(\ket{x},\ket{z}) + \lambda_y^\star(\ket{y},\ket{z})\nonumber
\end{align}
i.e., an inner product is conjugate-linear in the first
 argument.

\exercise*

Let $\{\ket{0},\ket{1}\}$ denotes the orthonormal bases of the vector representation.

We have
\begin{align}
\braket{w|v}&=(\bra{0}+\bra{1})(\ket{0}-\ket{1})\nonumber\\
&=\braket{0|0}-\braket{1|1}+\braket{1|0}-\braket{0|1}\nonumber\\
&=1-1+0-0\nonumber\\
&=0\nonumber
\end{align}
So $\ket{w}$ and $\ket{v}$ are orthogonal.

Their normalized forms are $\ket{w^\prime}=\frac{\ket{w}}{\left\|\ket{w}\right\|}=(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$ and $\ket{v^\prime}=\frac{\ket{v}}{\left\|\ket{v}\right\|}=(\frac{\sqrt{2}}{2},-\frac{\sqrt{2}}{2})$.

\exercise*
Let $\{\ket{w_i}\}$ denotes the original basis and $\{\ket{v_i}\}$ as the basis given by the Gram-Schmidt procedure.

According to definition, it is clear that they are normalized, i.e.,
\[\forall{i}.\;\braket{v_i|v_i}=\left\|v_i\right\|=1\]

Note that the inner product is conjugate-symmetry, we just need to check whether for all $1\leq k\leq d-1$ and $1\leq l\leq k$, $\ket{v_{k+1}}$ and $\ket{v_l}$ are orthogonal. And for simplicity we check numerators of the Gram-Schmidt form.

\begin{itemize}
    \item $\ket{v_2}$ and $\ket{v_1}$ are orthogonal because
    \[(\bra{w_2}-\braket{v_1|w_2}\bra{v_1})\ket{v_1}=\braket{w_2|v_1}-\braket{v_1|w_2}\braket{v_1|v_1}=0
    \]
    \item Assume that for all $1\leq i\leq k\;(2\leq k\leq d-1)$, $\{\ket{v_i}\}$ are orthonormal, then we have
    \begin{align}
        \forall 1\leq i\leq k.\;(\bra{w_{k+1}}-\sum_{j=1}^{k}\braket{v_j|w_{k+1}}\bra{v_j})\ket{v_i}&=\braket{w_{k+1}|v_i}-\sum_{j=1}^{k}\braket{v_j|w_{k+1}}\delta_{ij}\nonumber\\
        &=\braket{w_{k+1}|v_i}-\braket{v_i|w_{k+1}}\nonumber\\
        &=0\nonumber
    \end{align}
    i.e., $\{\ket{v_i}\}\cup\{\ket{v_{k+1}}\}$ are orthonormal.
    
\end{itemize}

By induction we prove that $\{\ket{v_i}\}$ is an an orthonormal basis.

\exercise*
\begin{itemize}
    \item $X=\ket{0}\bra{1}+\ket{1}\bra{0}$
    \item $Y=-i\ket{0}\bra{1}+i\ket{1}\bra{0}$
    \item $Z=\ket{0}\bra{0}-\ket{1}\bra{1}$
\end{itemize}

\exercise*
Let $A$ be the matrix representation for the operator $\ket{v_j}\bra{v_k}$, we have
\[\forall m\neq j\;\text{or}\;n\neq k.\;\bra{v_m}A\ket{v_n}=0\]
Note that $A\ket{v_n}$ is a linear combination of $\{\ket{v_i}\}$, hence $\bra{v_m}A\ket{v_n}=0$ if and only if $A_{mn}=0$.

Furthermore,
\[\bra{v_j}A\ket{v_k}=A_{jk}\braket{v_j|v_j}=A_{jk}=1\]

So $A$ is a $\text{rank}(V)\times \text{rank}(V)$ matrix, with $A_{jk}=1$ and other entries set to 0.

\exercise*
\begin{itemize}
    \item $\det|X-\lambda I|=\lambda^2-1=0\Rightarrow\lambda_{1,2}=\pm1$
    
        \subitem $\lambda_1=1$:\quad$(X-I)\ket{x}=0\Rightarrow$ a normalized eigenvector $\ket{x_1}=\frac{\sqrt{2}}{2}(1,1)$
        \subitem $\lambda_2=-1$:\;\;$(X+I)\ket{x}=0\Rightarrow$ a normalized eigenvector $\ket{x_2}=\frac{\sqrt{2}}{2}(1,-1)$
        
    Hence a diagonal representation is $X=\ket{x_1}\bra{x_1} - \ket{x_2}\bra{x_2}$
    \item $\det|Y-\lambda I|=\lambda^2-1=0\Rightarrow\lambda_{1,2}=\pm 1$
    
        \subitem $\lambda_1=1$:\quad$(Y-I)\ket{y}=0\Rightarrow$ a normalized eigenvector $\ket{y_1}=\frac{\sqrt{2}}{2}(1,i)$
        \subitem $\lambda_2=-1$:\;\;$(Y+I)\ket{y}=0\Rightarrow$ a normalized eigenvector $\ket{y_2}=\frac{\sqrt{2}}{2}(1,-i)$
        
    Hence a diagonal representation is $Y=\ket{y_1}\bra{y_1} - \ket{y_2}\bra{y_2}$
    \item $\det|Z-\lambda I|=\lambda^2-1=0\Rightarrow\lambda_{1,2}=\pm 1$
    
        \subitem $\lambda_1=1$:\quad$(Z-I)\ket{z}=0\Rightarrow$ a normalized eigenvector $\ket{z_1}=(1,0)$
        \subitem $\lambda_2=-1$:\;\;$(Z+I)\ket{z}=0\Rightarrow$ a normalized eigenvector $\ket{z_2}=(0,1)$
        
    Hence a diagonal representation is $Y=\ket{z_1}\bra{z_1} - \ket{z_2}\bra{z_2}$
\end{itemize}

\exercise*
\[\det (\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}-\lambda I)=(1-\lambda)^2=0\]
We get the only eigenvalue $\lambda=1$.

Let $(A-I)\ket{x}=0$, then get an eigenvector $\ket{x}=(0,1)$ and the rank of the eigenspace w.r.t. $\lambda=1$ is 1.

Note that
\[k\ket{x}\bra{x}=\begin{bmatrix} 0 & 0 \\ 0 & k \end{bmatrix}\neq\begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}\]

Hence the matrix is not diagonalizable.

\exercise*
Suppose $\ket{w}\in W$ and $\ket{v}\in V$.

Let operator $A=\ket{w}\bra{v}$ and $B=\ket{v}\bra{w}$. It is not difficult to see that $A$ maps from $V$ to $W$ and $B$ maps from $W$ to $V$.

\[\forall \ket{i}\in V.\;(A\ket{i})^\dagger=\braket{v|i}^\star\bra{w}=\braket{i|v}\bra{w}=\bra{i}A^\dagger=\bra{i}B\]

Hence $B=A^\dagger$, i.e., $(\ket{w}\bra{v})^\dagger=\ket{v}\bra{w}$.

\exercise*
Suppose $A_i$ maps from $V$ to $W$.

For all $\ket{v}\in V, \ket{w}\in W$,
\begin{align}
((\sum_i a_i A_i)^\dagger\ket{v},\ket{w})&=(\ket{v},\sum_i a_i A_i\ket{w})\nonumber\\
&=\sum_i a_i(\ket{v},A_i\ket{w})\nonumber\\
&=\sum_i a_i(A_i^\dagger\ket{v},\ket{w})\nonumber\\
&=((\sum_i a_i^\star A_i^\dagger)\ket{v},\ket{w})\nonumber
\end{align}
Hence $(\sum_i a_i A_i)^\dagger=\sum_i a_i^\star A_i^\dagger$.

\exercise*
Suppose $A$ maps from $V$ to $W$.

For all $\ket{v}\in V, \ket{w}\in W$,
\[
((A^\dagger)^\dagger\ket{v},\ket{w})=(\ket{v},A^\dagger\ket{w})=(A^\dagger\ket{w},\ket{v})^\star=(\ket{w},A\ket{v})^\star=(A\ket{v},\ket{w})
\]
Hence $(A^\dagger)^\dagger=A$.

\exercise*
Suppose $P$ maps from $V$ to its subspace $W$: \[P=\sum_{i=1}^k\ket{i}\bra{i},\]
where $\{\ket{i}\}$ is an orthonormal basis for $W$.
\begin{align}
    \forall \ket{v}\in V, P^2&=\sum_{i}\ket{i}\bra{i}\sum_{j}\ket{j}\bra{j}\nonumber\\
    &=\sum_{i,j}\ket{i}\braket{i|j}\bra{j}\nonumber\\
    &=\sum_{i,j}\delta_{ij}\ket{i}\bra{j}\nonumber\\
    &=\sum_{i}\ket{i}\bra{i}\nonumber\\
    &=P\nonumber
\end{align}

\exercise*
\begin{itemize}
    \item $\Rightarrow$:
    
    For any Hermitian matrix $A$, suppose it has an eigenvalue $\lambda$ and a corresponding eigenvector $\ket{x}$.
    
    We have
    \[\bra{x}A\ket{x}=\bra{x}\lambda\ket{x})=\lambda\braket{x|x}\]
    
    On the other side,
    \[\bra{x}A\ket{x}=\bra{x}A^\dagger\ket{x})=\bra{x}A\ket{x}^\star=\lambda^\star\braket{x|x}\]
    
    Note that $\braket{x|x}> 0$, so $\lambda=\lambda^\star$.i.e., $\lambda$ is real.
    \item $\Leftarrow$:
    
    Let $A$ be any normal matrix with real eigenvalues. As it is normal, it can be diagonalize as follows:
    \[A=\sum_i\lambda_i\ket{i}\bra{i}\]
    where $\{\ket{i}\}$ are eigenvectors corresponding to eigenvalues $\{\lambda_i\}$. 
    
    (Notice that the rank of an eigenspace may be greater than 1, but it does not matter.)
    
    Given that $\lambda_i$ is real, \[A^\dagger=(A^\star)^T=\sum_i\lambda_i^\star\ket{i}\bra{i}=\sum_i\lambda_i\ket{i}\bra{i}=A\]
    i.e., $A$ is Hermitian.
\end{itemize}
Combining the two parts above, a normal matrix is Hermitian if and only if it has real eigenvalues.
\exercise*
Let $U$ be a unitary matrix, and $\lambda$ as any of its eigenvalues. Then we have

\[(U\ket{v},U\ket{w})=(\lambda\ket{v},\lambda\ket{w})=\lambda\lambda^\star(\ket{v},\ket{w})\]

Also by definition
\[(U\ket{v},U\ket{w})=(\ket{v},\ket{w})\]

Hence 
\[\lambda\lambda^\star=1\]

Note that $\left\|\lambda\right\|\left\|\lambda^\star\right\|=1$ and $\left\|\lambda\right\|=\left\|\lambda^\star\right\|\geq0$, so $\left\|\lambda\right\|=1$.

In conclusion, all eigenvalues of $U$ have modulus 1.

\exercise*
\begin{itemize}
    \item $I$: It is clear that $I$ is Hermitian and unitary.
   \item $X$:
   \[X^\dagger=\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}=X\]
   \[X^\dagger X=\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}=I\]
   \item $Y$:
   \[Y^\dagger=\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}=Y\]
   \[Y^\dagger Y=\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}=I\]
   \item $Z$:
   \[Z^\dagger=\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}=Z\]
   \[Z^\dagger Z=\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}=\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}=I\]
\end{itemize}

\exercise*
Let $U=\sum_i\ket{w_i}\bra{v_i}$.

Note that $\ket{v_i}$ and $\ket{w_i}$ are both orthonormal bases, hence the operator $U$ is unitary.
\begin{align}
    A_{ij}^\prime=\bra{v_i}A\ket{v_j}&=\bra{v_i}U U^\dagger AU U^\dagger\ket{v_j}\nonumber\\
    &=\sum_k\braket{v_i|w_k}\bra{v_k} U^\dagger AU \sum_l\braket{w_l|v_j}\ket{v_l}\nonumber\\
    &=\sum_k\braket{v_i|w_k}\bra{v_k} \sum_m \ket{v_m}\bra{w_m} A \sum_n\ket{w_n}\bra{v_n} \sum_l\braket{w_l|v_j}\ket{v_l}\nonumber\\
    &=\sum_{k,l}\braket{v_i|w_k}\braket{w_l|v_j} \sum_{m,n} \delta_{km}A_{mn}^{\prime\prime} \delta_{nl}\nonumber\\
    &=\sum_{k,l}\braket{v_i|w_k}\braket{w_l|v_j} A_{kl}^{\prime\prime} \nonumber
\end{align}
which characterizes the relationship between $A^\prime$ and $A^{\prime\prime}$.

\exercise*
\begin{itemize}
    \item $\Rightarrow$:
    We also prove by induction on the dimension $d$ of $V$. The case $d=1$ is also trivial.
    
    Let $\lambda$ be an eigenvalue of $M$, $P$ the projector onto the $\lambda$ eigenspace, and $Q$ the projector onto the orthogonal complement. Then
    
    \[M=(P+Q)M(P+Q)=PMP+QMP+PMQ+QMQ\]
    Note that $QMP=0$ as $M$ takes the subspace $P$ into itself. Also we have
    \[PMQ=(Q M^\dagger P)^\star=(Q M P)^\star=0\]
    
    Next we prove that $QMQ$ is Hermitian:
    \[(QMQ)^\dagger=QM^\dagger Q=QMQ\]
    
    By induction, $QMQ$ is diagonal w.r.t. some orthonormal basis for the subspace $Q$, and $PMP$ is already diagonal w.r.t. some orthonormal basis for the $\lambda$ eigenspace.
    
    It follows that $M=PMP+QMQ$ is also diagonal, w.r.t. some orthonormal basis for $V$.
    
    \item $\Leftarrow$: It turns out that the statement does not hold conversely, in the Hermitian case.
    
\end{itemize}

\exercise*
Let $A$ by any Hermitian operator with eigenvalues $\lambda_1$ and $\lambda_2$, and corresponding eigenvectors $\ket{\lambda_1}$ and $\ket{\lambda_2}$. ($\lambda_1$ and $\lambda_2$ are real as $A$ is Hermitian.)

Note that $A\ket{\lambda_2}=\lambda_2\ket{\lambda_2}$ and $\bra{\lambda_1}A^\dagger=\lambda_1^\star\bra{\lambda_1}=\lambda_1\bra{\lambda_1}$,
\[\bra{\lambda_1}A\ket{\lambda_2}=\lambda_2\braket{\lambda_1|\lambda_2}\]
Also we have 
\[\bra{\lambda_1}A\ket{\lambda_2}=\bra{\lambda_1}A^\dagger\ket{\lambda_2}=\lambda_1\braket{\lambda_1|\lambda_2}\]
Combine the two equations above we get
\[(\lambda_1-\lambda_2)\bra{\lambda_1|\lambda_2}=0\]
As $\lambda_1 \neq \lambda_2$, $\ket{\lambda_1}$ and $\ket{\lambda_2}$ are necessarily orthogonal.

\exercise*
For simplicity we use $P$ to denote both the projector and the associated subspace.

Let $V$ be the total vector space and $Q$ the orthogonal complement of $P$.

For any non-zero vector $\ket{v}\in V$, it could be written in the following form:
\[\ket{v}=\ket{v_P}+\ket{v_Q}\]
where $\ket{v_P}\in P$ and $\ket{v_Q}\in Q$.

If $P\ket{v}=\lambda \ket{v}$ holds for some $\lambda$, we have
\begin{align}
    P\ket{v}&=P\ket{v_P}+P\ket{v_Q}\nonumber\\
    &=\ket{v_P}\nonumber\\
    &=\lambda\ket{v_P} + \lambda\ket{v_Q}\nonumber
\end{align}
Hence
\[(\lambda-1)\ket{v_P}+\lambda\ket{v_Q}=\vec 0\]
From exercise 2.22 we know that $\ket{v_P}$ and $\ket{v_Q}$ are orthonormal and they cannot be zero vectors at the same time, so there are only two possible cases:
\begin{itemize}
    \item $\lambda =0$: In this case $\ket{v}\in Q$
    \item $\lambda =1$: In this case $\ket{v}\in P$
\end{itemize}
In conclusion, the eigenvalues of P are all either 0 or 1.

\exercise*
For any positive operator $A$, we have
\[A=\frac{A+A^\dagger}{2}+i\frac{A-A^\dagger}{2i}\]
Let $B=\frac{A+A^\dagger}{2}$ and $C=\frac{A-A^\dagger}{2i}$, and it is clear that they are both Hermitian. Hence we have
\[\forall\ket{v}.\;\bra{v}A\ket{v}=\bra{v}B+iC\ket{v}=\bra{v}B\ket{v}+i\bra{v}C\ket{v}\geq 0\]
Note that $B$ and $C$ are Hermitian, $\bra{v}B\ket{v}$ and $\bra{v}C\ket{v}$ should both be real. Since the equation above holds for all $\ket{v}$, $C$ must be a zero operator. Therefore
\[A=A^\dagger\]
i.e., $A$ is Hermitian.

\exercise*
For any $\ket{v}$, suppose $A$ maps from $\ket{v}$ to $\ket{w}$.

We have
\[(\ket{v},A^\dagger A\ket{v})=\bra{v}A^\dagger A\ket{v}=\braket{w|w}\geq 0\]
By definition $A^\dagger A$ is positive.
\exercise*
\begin{itemize}
    \item 
    \[\ket{\psi}^{\otimes 2}=(\frac{\ket{0}+\ket{1}}{\sqrt{2}})\otimes(\frac{\ket{0}+\ket{1}}{\sqrt{2}})=\frac{1}{2}\ket{00}+\frac{1}{2}\ket{01}+\frac{1}{2}\ket{10}+\frac{1}{2}\ket{11}\]
    \[\ket{\psi}^{\otimes 2}=\begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}\otimes\begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}=\frac{1}{2}\begin{bmatrix} 1 \\ 1\\ 1\\ 1 \end{bmatrix}\]
    \item
    \begin{align}
        \ket{\psi}^{\otimes 3}=&(\frac{\ket{0}+\ket{1}}{\sqrt{2}})\otimes(\frac{\ket{0}+\ket{1}}{\sqrt{2}})\otimes(\frac{\ket{0}+\ket{1}}{\sqrt{2}})\nonumber\\
        =&\frac{\sqrt{2}}{4}\ket{000}+\frac{\sqrt{2}}{4}\ket{001}+\frac{\sqrt{2}}{4}\ket{010}+\frac{\sqrt{2}}{4}\ket{011}+\frac{\sqrt{2}}{4}\ket{100}+\frac{\sqrt{2}}{4}\ket{101}\nonumber\\
        &+\frac{\sqrt{2}}{4}\ket{110}+\frac{\sqrt{2}}{4}\ket{111}\nonumber
    \end{align}
    \[\ket{\psi}^{\otimes 3}=\begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}\otimes\begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}\otimes\begin{bmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{bmatrix}=\frac{\sqrt{2}}{4}\begin{bmatrix} 1 \\ 1\\ 1\\ 1\\ 1 \\ 1\\ 1 \\ 1 \end{bmatrix}\]
\end{itemize}

\exercise*
\begin{itemize}
\item
\[X\otimes Z=\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1 \\
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0\end{bmatrix}\]
\item
\[I\otimes X=\begin{bmatrix}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0\end{bmatrix}\]
\item
\[X\otimes I=\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0\end{bmatrix}\]
\end{itemize}

Note that $I\otimes X\neq X\otimes I$, hence the tensor product is not commutative, in general.
\exercise*
\begin{itemize}
    \item Distribution of transposition:
    \[(A\otimes B)^\text{T}=\begin{bmatrix}
    A_{11}B^\text{T} & A_{21}B^\text{T} & \cdots & A_{m1}B^\text{T}\\
    A_{12}B^\text{T} & A_{22}B^\text{T} & \cdots & A_{m2}B^\text{T}\\
    \vdots & \vdots & \vdots & \vdots\\
    A_{1n}B^\text{T} & A_{2n}B^\text{T} & \cdots & A_{mn}B^\text{T}
    \end{bmatrix}
    =\begin{bmatrix}
    A_{11} & A_{21} & \cdots & A_{m1}\\
    A_{12} & A_{22} & \cdots & A_{m2}\\
    \vdots & \vdots & \vdots & \vdots\\
    A_{1n} & A_{2n} & \cdots & A_{mn}
    \end{bmatrix}\otimes B^\text{T}=A^\text{T}\otimes B^\text{T}\]
    
    \item Distribution of complex conjugation:
    \[(A\otimes B)^\star=\begin{bmatrix}
    A_{11}^\star B^\star & A_{12}^\star B^\star & \cdots & A_{1n}^\star B^\star\\
    A_{21}^\star B^\star & A_{22}^\star B^\star & \cdots & A_{2n}^\star B^\star\\
    \vdots & \vdots & \vdots & \vdots\\
    A_{m1}^\star B^\star & A_{m2}B^\star & \cdots & A_{mn}^\star B^\star
    \end{bmatrix}
    =\begin{bmatrix}
    A_{11}^\star & A_{12}^\star & \cdots & A_{1n}^\star\\
    A_{21}^\star & A_{22}^\star & \cdots & A_{2n}^\star\\
    \vdots & \vdots & \vdots & \vdots\\
    A_{m1}^\star & A_{2n}^\star & \cdots & A_{mn}^\star
    \end{bmatrix}\otimes B^\star=A^\star\otimes B^\star\]
    
    \item Distribution of adjoint operation:
    
    Based on the two distribution laws above, we have
    \[(A\otimes B)^\dagger=((A\otimes B)^\star)^\text{T}=((A^\star\otimes B^\star)^\text{T}=A^\dagger\otimes B^\dagger\]
\end{itemize}

\exercise*
Let $U_1:V\to V^\prime$ be a unitary operator and $U_2:W\to W^\prime$ as another unitary operator.

Based on exercise 2.28 we know that $(U_1\otimes U_2)^\dagger=U_1^\dagger\otimes U_2^\dagger$. So

\[(U_1\otimes U_2)^\dagger(U_1\otimes U_2)=U_1^\dagger U_1\otimes U_2^\dagger U_2=I_1\otimes I_2=I_{V\otimes W}\]
Similarly we have $(U_1\otimes U_2)(U_1\otimes U_2)^\dagger=I_{V^\prime\otimes W^\prime}$, i.e., $U_1\otimes U_2$ is also unitary.

\exercise*
Let $A$ be a Hermitian operator on $V$ and $B$ as another Hermitian operator on $W$.

Based on exercise 2.28 we know that
\[(A\otimes B)^\dagger=A^\dagger\otimes B^\dagger=A\otimes B\]
i.e., $A\otimes B$ is also Hermitian.

\exercise*
Let $A$ be a positive operator on $V$ and $B$ as another positive operator on $W$. 

Note that $\ket{v_i}\otimes\ket{w_j}$ forms an orthonormal basis for $V\otimes W$, hence for any $\ket{u}$ in $V\otimes W$, it is a linear combination of $\{\ket{v_i}\otimes\ket{w_j}\}$:
\[\ket{u}=\sum_{k,l}a_{kl}\ket{v_k}\otimes\ket{w_l}\]
Consider the natural inner product on $V\otimes W$ and apply linearity:
\begin{align}
(\ket{u},(A\otimes B)\ket{u})&=(\sum_{k,l}a_{kl}\ket{v_k}\otimes\ket{w_l}, \sum_{k,l}a_{kl}A\ket{v_k}\otimes B\ket{w_l})\nonumber\\
&=\sum_{k,l}\left\|a_{kl}\right\|^2\bra{v_k}A\ket{v_k}\bra{w_l}B\ket{w_l}\nonumber\\
&\geq 0\nonumber
\end{align}
Hence $A\otimes B$ is also positive.

\exercise*
Let $P_V=\sum_i \ket{v_i}\bra{v_i}$ be a projector on $V$ and $P_W=\sum_j\ket{w_j}\bra{w_j}$ as another projector on $W$.
For simplicity we have $P_V$ and $P_W$ also denote the corresponding subspace.

It is clear that
\[P_V\otimes P_W=\sum_{i,j}\ket{v_i}\otimes\ket{w_j}\bra{v_i}\otimes\bra{w_j}\]

Note that $\ket{v_i}\otimes\ket{w_j}$ forms a orthonormal basis for $V\otimes W$, therefore $P_V\otimes P_W$ is also a projector on $V\otimes W$, w.r.t. the subspace $P_V\otimes P_W$.

\exercise*
We prove the statement inductively.
\begin{itemize}
    \item When $n=1$, it holds that
    \[H^{\otimes 1}=\frac{1}{\sqrt{2}}(\ket{0}\bra{0}+\ket{0}\bra{1}+\ket{1}\bra{0}-\ket{1}\bra{1})=\frac{1}{\sqrt{2}}\sum_{x_1,y_1=0,1}(-1)^{x\cdot y}\ket{x_1}\bra{y_1})\]
    \item Assume that for any $n\geq 1$, 
    \[H^{\otimes n}=\frac{1}{\sqrt{2^n}}\sum_{x_n,y_n=0,1,\dots, 2^n -1}(-1)^{x_n\cdot y_n}\ket{x_n}\bra{y_n}\]
    Note that every time we tensor with $H$, the basis changes from $\ket{x_n}$ to $\ket{x_{n+1}}$, with $\ket{i}\leftarrow\ket{i}\otimes\ket{0}$ and $\ket{i+2^n}\leftarrow\ket{i}\otimes\ket{1}$ for all $0\leq i< 2^n$.
    
    Hence we have
    \[H^{\otimes n+1}=H^{\otimes n}\otimes H=\frac{1}{\sqrt{2^{n+1}}}\sum_{x_{n+1},y_{n+1}=0,1,\dots,2^{n+1}-1}(-1)^{x_{n+1}\cdot y_{n+1}}\ket{x_{n+1}}\bra{y_{n+1}}\]
\end{itemize}

By induction we can prove that the equation of $H^{\otimes n}$ always holds.

The matrix representation for $H^{\otimes 2}$ is
\[\begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & -1 & 1 & -1 \\
1 & 1 & -1 & -1 \\
1 & -1 & -1 & 1\end{bmatrix}\]

\exercise*
Let $A=\begin{bmatrix}
4 & 3\\
3 & 4\end{bmatrix}$.
\[\det(A-\lambda I)=0\Rightarrow\lambda_1=1,\lambda_2=7\]
The corresponding normalized eigenvectors are
\[\ket{\lambda_1}=\frac{1}{\sqrt{2}}\begin{bmatrix}
1\\
-1\end{bmatrix}\]
\[\ket{\lambda_2}=\frac{1}{\sqrt{2}}\begin{bmatrix}
1\\
1\end{bmatrix}\]
Then
\[A=\ket{\lambda_1}\bra{\lambda_1}+7\ket{\lambda_2}\bra{\lambda_2}\]
The square root of A is
\[\sqrt{A}=\ket{\lambda_1}\bra{\lambda_1}+\sqrt{7}\ket{\lambda_2}\bra{\lambda_2}=\frac{1}{2}\begin{bmatrix}
1+\sqrt{7} & -1+\sqrt{7}\\
-1+\sqrt{7} & 1+\sqrt{7}\end{bmatrix}\]
The logarithm of A is
\[\log(A)=\log(0)\ket{\lambda_1}\bra{\lambda_1}+\log(7)\ket{\lambda_2}\bra{\lambda_2}=\frac{\log(7)}{2}\begin{bmatrix}
1 & 1\\
1 & 1\end{bmatrix}\]

\exercise*
By definition we get
\[\vec v\cdot\vec \sigma=\begin{bmatrix}
v_3 & v_1-iv_2\\
v_1+iv_2 & -v_3\end{bmatrix}\]
Note that $\vec v$ is a unit vector,
\begin{align}\det(\vec v\cdot\vec \sigma-\lambda I)=0&\Rightarrow\lambda^2=v_1^2+v_2^2+v_3^2=1\nonumber\\
&\Rightarrow\lambda_{1,2}=\pm1\nonumber
\end{align}
Note that $\vec v\cdot\vec\sigma$ is Hermitian, hence it could be diagonalized as follows:
\[\vec v\cdot\vec \sigma=\ket{\lambda_1}\bra{\lambda_1}-\ket{\lambda_2}\bra{\lambda_2}\]
where $\ket{\lambda_{1,2}}$ are the corresponding normalized eigenvectors. (Do not need to explicitly solve them since we already know that $\vec v\cdot\vec\sigma$ is diagonalizable.)

Furthermore,
\[\ket{\lambda_1}\bra{\lambda_1}-\ket{\lambda_2}\bra{\lambda_2}=I\]

Based on the definition of operator functions and applying Euler's equation, we have
\begin{align}
    \exp(i\theta\vec v\cdot\vec\sigma)&=e^{i\theta}\ket{\lambda_1}\bra{\lambda_1}+e^{-i\theta}\ket{\lambda_2}\bra{\lambda_2}\nonumber\\
    &=(\cos{\theta}+i\sin{\theta})\ket{\lambda_1}\bra{\lambda_1}+(\cos{\theta}-i\sin{\theta})\ket{\lambda_2}\bra{\lambda_2}\nonumber\\
    &=\cos{\theta}(\ket{\lambda_1}\bra{\lambda_1}+\ket{\lambda_2}\bra{\lambda_2})+i\sin{\theta}(\ket{\lambda_1}\bra{\lambda_1}-\ket{\lambda_2}\bra{\lambda_2})\nonumber\\
    &=\cos{\theta}I+i\sin{\theta\vec v\cdot\vec\sigma}\nonumber
\end{align}

\exercise*
\begin{itemize}
    \item $\text{tr}(X)=0$
    \item $\text{tr}(Y)=0$
    \item $\text{tr}(Z)=1-1=0$
    
\end{itemize}

\exercise*
Suppose both $A$ and $B$ act on $V$ which has a orthonormal basis $\ket{v_i}$.
\begin{align}
    \text{tr}(AB)&=\sum_i\bra{v_i}AB\ket{v_i}\nonumber\\
    &=\sum_i\bra{v_i}AIB\ket{v_i}\nonumber\\
    &=\sum_{i,j}\bra{v_i}A\ket{v_j}\bra{v_j}B\ket{v_i}\nonumber\\
    &=\sum_{i,j}\bra{v_j}B\ket{v_i}\bra{v_i}A\ket{v_j}\nonumber\\
    &=\sum_j\bra{v_j}BA\ket{v_j}\nonumber\\
    &=\text{tr}(BA)\nonumber
\end{align}

\exercise*
Suppose both $A$ and $B$ act on $V$ which has an orthonormal basis $\ket{v_i}$.
\begin{align}
    \text{tr}(A+B)&=\sum_i\bra{v_i}A+B\ket{v_i}\nonumber\\
    &=\sum_i\bra{v_i}A\ket{v_i}+\sum_i\bra{v_i}B\ket{v_i}\nonumber\\
    &=\text{tr}(A)+\text{tr}(B)\nonumber
\end{align}

Note that an inner product is linear in its second argument,
\begin{align}
    \text{tr}(zA)&=\sum_i\bra{v_i}zA\ket{v_i}\nonumber\\
    &=z\sum_i\bra{v_i}A\ket{v_i}\nonumber\\
    &=z\text{tr}(A)\nonumber
\end{align}

\exercise*
\subsection{}
Let $A,B,C$ be operators in $L_V$.
\begin{itemize}
    \item Based on exercise 2.38 we have
    \begin{align}
        (A,z_bB+z_cC)&=\text{tr}(A^\dagger (z_bB+z_cC))\nonumber\\
        &=\text{tr}(z_b A^\dagger B)+\text{tr}(z_c A^\dagger C)\nonumber\\
        &=z_b\text{tr}(A^\dagger B)+z_c\text{tr}(A^\dagger C)\nonumber\\
        &=z_b(A,B)+z_c(A,C)\nonumber
    \end{align}
    where $z_b$ and $z_c$ are arbitary complex numbers.
    
    i.e., $(\cdot,\cdot)$ is linear in its second argument.
    \item Conjugate-symmetry:
    \[(A,B)=\text{tr}(A^\dagger B)=\sum_i \bra{i}A^\dagger B\ket{i}=(\sum_i \bra{i}B^\dagger A\ket{i})^\star=(B,A)^\star\]
    \item 
    \begin{align}
    (A,A)=\text{tr}(A^\dagger A)&=\sum_i\bra{i}A^\dagger A\ket{i}\nonumber\\
    &=\sum_i\bra{i}A^\dagger I A\ket{i}\nonumber\\
    &=\sum_i\bra{i}A^\dagger (\sum_j \ket{j}\bra{j})A\ket{i}\nonumber\\\
    &=\sum_{i,j}\bra{i}A^\dagger \ket{j}\bra{j}A\ket{i}\nonumber\\
    &=\sum_{i,j}\bra{j}A \ket{i}^\star\bra{j}A\ket{i}\nonumber\\
    &=\sum_{i}\left\|\bra{i}A\ket{i}\right\|^2\nonumber\\
    &\geq 0\nonumber
    \end{align}
    with equality if and only if $A$ is a zero operator.
\end{itemize}
\subsection{}
Let $\ket{i}$ be an orthonormal basis for $V$.

It is clear that $\ket{i}\bra{j}$ are linearly independent. And for any operator in $L_V$,
it could be written in the outer product form, i.e., linear combination of $\ket{i}\bra{j}$.

Hence $\ket{i}\bra{j}$ forms a basis for $L_V$. So

\[\text{rank}(L_V)=\|\{\ket{i}\bra{j}\}\|=d^2\]
\subsection{}
Note that the basis given above is countable, we get a set of basis namely $\ket{l_k}$, where 
\[\ket{l_k}\equiv\ket{i}\bra{j}\quad(i=\lfloor k / d \rfloor -1,\;j = k\;\text{mod}\;n)\]

Apply the Gram–Schmidt procedure:

\begin{itemize}
    \item Define $\ket{l_1^\prime}=\frac{\ket{l_1}}{\|\ket{l_1}\|}$
    \item For $1\leq k \leq d^2 -1$ define $\ket{l_{k+1}^\prime}$ inductively by
    \[\ket{l_{k+1}^\prime}=\frac{\ket{l_{k+1}}-\sum_{i=1}^k\braket{l_i^\prime|l_{k+1}}\ket{l_i^\prime}}{\|\ket{l_{k+1}}-\sum_{i=1}^k\braket{l_i^\prime|l_{k+1}}\ket{l_i^\prime}\|}\]
\end{itemize}

The vectors $\ket{l_1^\prime},\ket{l_2^\prime},\dots,\ket{l_{d^2}^\prime}$ form an orthonormal basis for $L_V$. 

\exercise*
\begin{itemize}
    \item 
    \[[X,Y]=XY-YX=2\begin{bmatrix}i & 0\\0&-i\end{bmatrix}=2iZ\]
    \item
    \[[Y,Z]=YZ-ZY=2\begin{bmatrix}0 & i\\i&0\end{bmatrix}=2iX\]
    \item
    \[[Z,X]=ZX-XZ=2\begin{bmatrix}0 & 1\\-1&0\end{bmatrix}=2iY\]
\end{itemize}

\exercise*

\exercise*
\[\frac{[A,B]+\{A,B\}}{2}=\frac{AB-BA+AB+BA}{2}=AB\]

\exercise*
Based on exercise 2.41, we have
\[\{\sigma_j,\sigma_k\}=2\delta_{jk}I\]
Hence
\begin{align}
    \sigma_j\sigma_k&=\frac{\{\sigma_j,\sigma_k\}+[\sigma_j,\sigma_k]}{2}\nonumber\\
    &=\frac{2\delta_{jk}I+2i\sum_{l=1}^3\epsilon_{jkl}\sigma_l}{2}\nonumber\\
    &=\delta_{jk}I+i\sum_{l=1}^3\epsilon_{jkl}\sigma_l\nonumber
\end{align}

\exercise*
Given that $[A,B]=0$ and $\{A,B\}=0$, we have
\[AB=BA\]
\[AB=-BA\]
Multiply by $A^{-1}$, we get
\[ABA^{-1}=BAA^{-1}=B\]
\[ABA^{-1}=-BAA^{-1}=-B\]
Note that $B=-B$, so $B$ must be 0.

\exercise*
\[[A,B]^\dagger=(AB-BA)^\dagger=B^\dagger A^\dagger-A^\dagger B^\dagger=[B^\dagger,A^\dagger]\]

\exercise*
\[[A,B]=AB-BA=-(BA-AB)=-[B,A]\]

\exercise*
\[i[A,B]=i(AB-BA)\]
Given that $A=A^\dagger$ and $B=B^\dagger$ as they are Hermitian,
\[(i[A,B])^\dagger=-i[A,B]^\dagger=-i(B^\dagger A^\dagger-A^\dagger B^\dagger)=i(B A-A B)=i[A,B]\]
Hence $i[A,B]$ is also Hermitian.

\exercise*

\exercise*

\exercise*

\exercise*
\[H^\dagger=\frac{1}{\sqrt{2}}
\begin{bmatrix}
1&1\\
1&-1
\end{bmatrix}
=H
\]
\[H^\dagger H=H H^\dagger=\frac{1}{2}
\begin{bmatrix}
2&0\\
0&2
\end{bmatrix}=I\]
Hence $H$ is unitary.

\exercise*
Note that $H=H^\dagger$, so
\[H^2=H H^\dagger=I\]

\exercise*
\[\det(H-\lambda I)=0\Rightarrow \lambda_{1,2}=\pm\frac{\sqrt{5}}{2}\]

\exercise*
Note that $A$ and $B$ commute, so they could be diagonalized w.r.t. an orthonormal basis:
\[A=\sum_{i}a_{i}\ket{i}\bra{i}\]
\[B=\sum_{i}b_{i}\ket{i}\bra{i}\]
By definition of operator functions, we have
\begin{align}
    \exp(A)\exp(B)&=\sum_{i}\exp(a_{i})\ket{i}\bra{i}\sum_{j}\exp(b_{j})\ket{j}\bra{j}\nonumber\\
    &=\sum_{i,j}\exp(a_{i}+b_{j})\ket{i}\delta_{ij}\bra{j}\nonumber\\
    &=\sum_i \exp(a_{i}+b_{i})\ket{i}\bra{i}\nonumber\\
    &=\exp(A+B)\nonumber
\end{align}

\exercise*
Note that the Hamiltonian is Hermitian and hence diagonalizable,
\[H=\sum_E E\ket{E}\bra{E}\]
where $E$ are real eigenvalues.

We get
\[U(t_1,t_2)=\exp[\frac{-iH(t_2-t_1)}\hbar]=\sum_E \exp[\frac{-iE(t_2-t_1)}{\hbar}]\ket{E}\bra{E}\]
and
\[U^\dagger(t_1,t_2)=\sum_E\exp[\frac{iE(t_2-t_1)}{\hbar}]\ket{E}\bra{E}\]

Therefore
\[U(t_1,t_2)U^\dagger(t_1,t_2)=\sum_{E_1,E_2}\exp[\frac{iE_2(t_2-t_1)-iE_1(t_2-t_1)}{\hbar}]\ket{E_1}\delta_{E_1,E_2}\bra{E_2}=\sum_E\ket{E}\bra{E}=I\]
and similarly $U^\dagger(t_1,t_2)U(t_1,t_2)=I$. i.e., $U(t_1,t_2)$ is unitary.

\exercise*
\begin{itemize}
    \item $\Rightarrow$:
    
    Note that $U$ is normal ($UU^\dagger=U^\dagger U=I$) and hence diagonalizable,
    \[U=\sum_E E\ket{E}\bra{E}\]
    where all eigenvalues $E$ (not necessarily real) have modulus 1 and can be written in the following form:
    \[E=\exp(i\theta_E)\]
    for some real $\theta_E$.
    
    
    We have
    \[K=-i\log(U)=\sum_E -i\log(E)\ket{E}\bra{E}=\sum_E \theta_E\ket{E}
    \bra{E}\]
    \[K^\dagger=\sum_E \theta_E\ket{E}\bra{E}=K\]
    
    Therefore $K\equiv-i\log(U)$ is Hermitian for any unitary $U$.
    \item $\Leftarrow$:
    
    Note that $K$ is Hermitian and hence diagonalizable,
    \[K=\sum_E E\ket{E}\bra{E}\]
    where all eigenvalues $E$ are real numbers and eigenvectors $\ket{E}$ are normalized.
    
    We have
    \[U=\exp(iK)=\sum_E \exp(iE)\ket{E}\bra{E}\]
    \[U^\dagger=\sum_E \exp(-iE)\ket{E}\bra{E}\]
    \[U^\dagger U=U U^\dagger=\sum_E \exp(iE-iE)\ket{E}\bra{E}=\sum_E\ket{E}\bra{E}=I\]
    Therefore $U\equiv \exp(iK)$ is unitary for any Hermitian $K$.
\end{itemize}

\end{document}